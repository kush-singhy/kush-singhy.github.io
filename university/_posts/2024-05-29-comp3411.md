---
layout: post
title:  "COMP3411 Notes"
date:   2024-05-29 18:06:02 +1000
category: university
---
## Lecture Notes


# Week 1

- Turing Test:
    - A human interrogates/converses with the computer via a teletype
    - The aim is for the computer to imitate a human well enough to fool the human interrogator

### Agent Model

- An agent has sensors which make perceptions from the environment and actuators which allow it to perform actions to the environment
- An agent is a **function** from PERCEPT SEQUENCES to ACTIONS
    - An ideal rational agent would pick actions which maximise the performance measure
- The PEAS model of an Agent:
    - Performance measure
    - Environment
    - Actuators
    - Sensors
- **Performance Measure**: Scoring mechanism of the agent
- **Environment**: the surroundings of the agent
    - Simulated vs Embodied
    - Static vs Dynamic
    - Discrete vs Continuous
    - Fully Observable vs Partially Observable
    - Deterministic vs Stochastic
    - Episodic vs Sequential
    - Single vs Multi-agent
- **Actuator**: ‘part’ responsible for the output
- **Sensor**: ‘part’ that receives the output

### Environment Types

| Simulated: A separate program is used to simulate an environment, feed percepts to agents, evaluate performance, e.g. chess | Situated: The agents acts directly on the environment, e.g. robocup
Embodied: The agent has a physical body in the world, e.g. robocup |
| Static: The environment does not change while the agent is thinking, e.g. chess | Dynamic: The environment may change while the agent is thinking, e.g. robocup
| Discrete: There are only a finite or countable number of discrete percepts/actions | Continuous: States, percepts or actions can vary continuously |
| Fully observable: Agent percepts contains all relevant information about the world | Partially observable: Some relevant information is hidden from the agent |
| Deterministic: The current state uniquely determines the next state | Stochastic: There is some random element involved |
| Episodic: Every actions by the agent is evaluated independently  | Sequential: The agent is evaluated based on a long sequence of actions |
| Single-agent: e.g. Sudoku, Rubik’s cube | Multi-agent: multiple agents |
| Known: The rules of the game, the physics and dynamics of the environment are known to the agent | Unknown |

**Dice Games**: Simulated, Static, Discrete, Fully observable, Stochastic, Sequential, Multi-agent

### Agents

Reactive Agent:
![Untitled]({{ "/images/comp3411images/Untitled.png" | absolute_url }})

- Choose the next action based on what they currently perceive using a set of rules
- Limits:
    - No memory or state

Model-based Agent:

![Untitled]({{ "/images/comp3411images/Untitled%201.png" | absolute_url }})

- Can keep a map of the places it has visited and remember what it perceived there
- Limits:
    - Can look into the past but not the future

**Planning Agent:**

![Untitled]({{ "/images/comp3411images/Untitled%202.png" | absolute_url }})

Learning Agent:

![Untitled]({{ "/images/comp3411images/Untitled%203.png" | absolute_url }})

- Not a separate module but rather a set of techniques for improving the existing modules
- Learning is necessary because:
    - may be difficult or even impossible for a human to design all aspects of the system by hand
    - the agent may need to adapt to new situations without being re-programmed by a human

### Constraint Satisfaction Problems

- CSPs are defined by a set of variables X, each with a domain D, of possible values, and a set of constraints C.
- The aim is to find an assignment of variables from the domains in such a way that none of the constraints are violated.
- Example:

![Untitled]({{ "/images/comp3411images/Untitled%204.png" | absolute_url }})

- Real world CSPs:
    - Assignment problems, e.g. Who teaches what class?
    - Timetabling problems
    - Transport scheduling
- Hashiwokakero:
    - Islands with numbers on them. Need to connect all islands using planks, and the number of planks going into each island must equal the number on the island itself.

![Untitled]({{ "/images/comp3411images/Untitled%205.png" | absolute_url }})

- Types of constraints:
    - Unary - involving a single variable
    - Binary - involving pairs of variables
    - Higher-order - involving 3 or more variables
    - Inequality
    - Soft constraints - preferences
- Path search vs CSPs
    - In CSPS the difficult part is knowing the final state but knowing how to get there is easy
    - In path search problems knowing the final state is easy but the difficult part is knowing how to get there
- Backtracking: Assigning values in a sequence until a constraint is violated, and then going back to the most recently assigned variable and assigning it a new one
- Depth first search:
    - Initial state: the empty assignment
    - Successor function: assign a value to an unassigned variable that does not conflict with previously assigned variables
    - Goal test: all variables have been assigned a value and no constraints are violated
- Search space for DFS:
    - If there are n variables, every solution will occur at exactly depth n
    - Variable assignments are commutative (order doesn’t matter)
- Improvements: Heuristics can give gains in speed. E.g:
    - Which variable should be assigned next?
    - In what order should the values be tried?
    - Can we detect inevitable failure early?

### General Purpose Heuristics:

- **Minimum Remaining Values (MRV)**: Choose the variable with the fewest legal values
    - After colouring WA Red, we choose either NT or SA next because these have two remaining values while the other states have three.

![Untitled]({{ "/images/comp3411images/Untitled%206.png" | absolute_url }})

- **Degree Heuristic**: As a tie-breaker among variables with the same minimum MRV value, we introduce the Degree Heuristic. Choose the variable which applies the most constraints on remaining variables
    - SA is the best region to color first, because it interacts with 5 other variables.

![Untitled]({{ "/images/comp3411images/Untitled%207.png" | absolute_url }})

- **Least Constraining Value**: Given a variable, choose the least constraining value. The one that rules out the fewest number of values in the remaining variables.
    - Suppose we colour WA Red and then assign a colour for Queensland. If Q in Red, then two values remain for NT and SA. If we colour it Green or Blue, only one value will remain for NT, SA. Therefore, Red is preferred.

![Untitled]({{ "/images/comp3411images/Untitled%208.png" | absolute_url }})

- **Forward checking**: Keep track of remaining legal values for unassigned variables. When any variable has no legal values remaining, prune off that part of the search tree, and backtrack.
- **Constraint propagation**: Propagating information from assigned to unassigned variables. Repeatedly enforces constraints locally.
- **Arc Consistency**:
    - We look at all interacting variables to make sure they are consistent.
    - X → Y is consistent if for every remaining value for X there is some remaining value for Y that is compatible with it.
    - This detects failure earlier than forward checking and can speed up searches.

- **Local Search/Iterative Improvement**: Assign all variables randomly in the beginning and then change one variable at a time, trying to reduce constraints violations at each step.

![Untitled]({{ "/images/comp3411images/Untitled%209.png" | absolute_url }})

- **Hill climbing by min-conflicts**: Randomly select any conflicted variable and choose the value that violates the fewest constraints.
    - Hill Climbing may sometimes get stuck in a Local Minimum – where constraints are still violated, but a change to any one variable would result in even more constraints being violated.
    - One way to deal with local minima is Random Re-Start Hill Climbing. If we detect that we are stuck in a local minimum, we start again from a random initial state.
- **Simulated Annealing**: A method for finding a 'good' solution to an optimisation problem.
    - Stochastic hill climbing based on difference between the number of constraints violated in the previous state (*h0*) and new state (*h*1*)*.
    - If *h*1 *< h0*, definitely make the change
    - Otherwise, make the change with probability
        
        ![Untitled]({{ "/images/comp3411images/Untitled%2010.png" | absolute_url }})
        
        - where *T* is a “temperature” parameter.
    - Reduces to ordinary hill climbing when *T*=0
    - Becomes totally random search as *T*→∞
    - Sometimes, we gradually decrease the value of *T* during the search
- Randomly generated CSPs tend to be easy if there are very few or very many constraints. They become extra hard in a narrow range of the ratio.

![Untitled]({{ "/images/comp3411images/Untitled%2011.png" | absolute_url }})

---

# Week 2

### Reactive Agents

![Untitled]({{ "/images/comp3411images/Untitled.png" | absolute_url }})

- Choose the next action based on what they currently perceive using a set of rules
- Unable to remember, plan or logically reason
- Robots abandon the traditional horizontal decomposition for a vertical decomposition (subsumption architecture)
- Horizontal:

![Untitled]({{ "/images/comp3411images/Untitled%2012.png" | absolute_url }})

- Vertical

![Untitled]({{ "/images/comp3411images/Untitled%2013.png" | absolute_url }})

- Each layer in the vertical decomp is a behaviour:
    - Low level behaviours like “avoid collisions” are reactive
    - Mid-level behaviours like “build maps” make use of the world model
    - High-level behaviours make use of world model and planning

### Path Search

- A **planning agent** can use search techniques to plan several steps ahead in order to achieve its goals
- **Search strategies:**
    - Uninformed - can only distinguish goal states from non-goal states
    - Informed - use heuristics to try to get closer to the goal
- Example: Romania Street Map

![Untitled]({{ "/images/comp3411images/Untitled%2014.png" | absolute_url }})

- Single-state task specification - A task is specified by states and actions:
    - State space - e.g. all cities
    - Initial state - e.g. Arad
    - Actions or Operators (or successor function) - e.g. Arad → Zerind
    - Goal test - e.g. Bucharest
    - Path cost - e.g. sum of distances
- Real world is absurdly complex, so we need to abstract the state space for problem solving.
    - Toy problems - concise exact description
    - Real world problems - don’t have a single agreed description

### Path Search Algorithms

- **Search**: Finding state-action sequences that lead to desirable states.
    - Basic idea: offline, simulated exploration of state space by generating successors of already explored states
- Generating Action Sequences:
    
    1. Start with a priority queue consisting of just the initial state.
    2. Choose a state from the queue of states which have been generated but not yet expanded.
    3. Check if the selected state is a Goal State. If it is, STOP (solution has been found).
    4. Otherwise, expand the chosen state by applying all possible transitions and generating all its children.
    5. If the queue is empty, Stop (no solution exists).
    6. Otherwise, go back to Step 2.
    

### **Data Structures for a Node:**

- A node data structure with five components
    - Corresponding state
    - Parent node
    - Operator that was applied to generate the current node
    - Depth
    - Path cost

### **Search Trees**

- **Search Tree**: Superimposed over the state space
- **Root**: search node corresponding to the initial state
- **Leaf nodes**: correspond to states that have no successors in the tree because they were not expanded or generated no new nodes
- Frontier: collection of nodes waiting to be expanded. It can be implemented as a priority queue with the following operations:
    - Make-Queue(items) - creates queue
    - Boolean Empty(Queue) - returns true if no items in queue
    - Remove-Front(Queue) - removes the item at the front of the queue and returns it
    - Queueing-Function(Items, Queue) - inserts new items into the queue
- A strategy is defined by picking the order of node expansion
- Comparing search strategies:
    - Completeness - does it always find a solution if one exists?
    - Time complexity - number of nodes generated
    - Space complexity - maximum number of nodes in memory
    - Optimality - does it always find the least-cost solution?
- Time & Space complexity are measured in terms of:
    - b - maximum branching factor of the search tree
    - d - depth of the least cost solution
    - m - maximum depth of the state space
- Complexity
    - Using O(n) notation.
    - O() abstracts over constant factors. It is a good compromise between precision and ease of analysis

### Uninformed Search

- **Breadth-First Search**:
    - Expand each node at every level.
    - Add all new nodes the the **back** of the queue
- **Uniform-Cost Search:**
    - Expand root first, then expand least-cost unexpanded node
    - *Queueing Function* - insert nodes in order of increasing path cost
    - Reduces to breadth first search when all actions have same cost
- **Depth-First Search:**
    - Using a stack instead of a queue.
    - Expand down one path all the way down.
    - Uses very little memory, but may have a much higher time complexity
- **Depth-Limited Search:**
    - Impose cutoff on the maximum depth of path
- **Iterative Deepening Search:**
    - Doing a depth first search to each depth, then starting over
- **Bidirectional Search:**
    - Search both forward from the initial state and backward from the goal state.
    - Issues: Searching backwards means generating predecessors which can be difficult, there can be several goals

### Complexity Results for Uninformed Search

![Untitled]({{ "/images/comp3411images/Untitled%2015.png" | absolute_url }})

- b = branching factor
- d = depth of shallowest solution
- m = maximum depth of tree
- k = depth limit
- 1 = complete if b is finite
- 2 = complete if b is finite and step costs > 0
- 3 = optimal if actions all have the same cost

### Informed Search

- **Informed or Heuristic Search** - search strategies that incorporate an estimate of the distance to the goal into the function
- Heuristic function are problem specific functions that provide an estimate of solution cost
- **Best-First Search** is a general term for more sophisticated algorithms which use an evaluation function to try to guess which would be the best node to expand next.
- **Uniform Cost Search:**
    - Uses an evaluation function *f()* to order the nodes in the queue
    - f(n) = cost g(n) of path from root to node *n*
    - Expand root first, then expand least-cost unexpanded node
    - Reduces to BFS when all actions have the same cost
- **Greedy Search:**
    - f(n) = estimate h(n) of cost from node n to goal
    - Expands whichever node is estimated to be closest to the goal
    - Tries to complete as much of the solution as possible without worrying about long-term consequences.
    - Has the same deficits as Depth first search. However, a good heuristic can save time and memory costs.
- **A* Search:**
    - Evaluation function: f(n) = g(n) + h(n)
    - Estimated total cost of cheapest solution through node n
    - Preserves efficiency of greedy search but avoids expanding paths that are already expensive
    - Is optimal and complete if **h() is admissible**, meaning it never overestimates the cost to reach the goal.

### Complexity Results for Informed Search

|  | Greedy Search | A* Search |
| --- | --- | --- |
| Time | O(b^m) |  |
| Space | O(b^m) |  |
| Complete? | No | Yes* |
| Optimal | No | Yes* |

### Choosing Heuristic Functions

- Dominance:
    - If h2(n) > h1(n) for all n, then h2 dominates h1 and is better for search. The aim is to make the heuristic as large as possible without exceeding h*().
- Admissible heuristics can often be derived from the **exact** solution cost of a simplified or “relaxed” version of the problem. (i.e. with some of the constraints weakened or removed)
- Composite Heuristic Functions:

![Untitled]({{ "/images/comp3411images/Untitled%2016.png" | absolute_url }})

---

# Week 3

### Games

- Types:
    - Discrete games (e.g. chess, monopoly, poker)
    - Continuous games (e.g. pool, robocup soccer)
- Unpredictable opponent - the solution is a strategy
    - must respond to every possible opponent reply
- Time limits - must rely on approximation as there is a trade-off between speed and accuracy
- Zero sum games - where one players win implies the other players loss

### Game Search Types

- **Minimax**: Fundamental adversarial search
- **Alpha-Beta Pruning**: Faster variant of Minimax
- **Negamax**: A convenient way of implementing both Minimax and Alpha-Beta search
- **Expectimax**: Includes Chance nodes and is needed for stochastic games
- **Monte Carlo Tree Search**: Useful for some games with a very high branching factor.

### Minimax

- Minimax can be implemented in a way that has the same basic structure as Depth First Search.
- At each leaf  node, it returns either a terminal value (if the game has ended) or a heuristic value.
- At each interior node, it recursively computes the values of all the child nodes, and returns either the minimum or maximum of these values.
- Algorithm:

```c
function minimax( node, depth )
    if node is a terminal node or depth == 0
        return heuristic value of node
    if we are to play at node
        let α = − ∞
        foreach child of node
            let α = max( α, minimax( child, depth-1 ))
        return α
    else // opponent is to play at node
        let β = + ∞
        foreach child of node
            let β = min( β, minimax( child, depth-1 ))
        return β

```

- The above formulation of Minimax assumes that all nodes are evaluated with respect to a fixed player (e.g. White in Chess).
- If we instead assume that each node is evaluated with respect to the player whose turn it is to move, we get a simpler formulation known as **Negamax**.
- Algorithm:

```c
function negamax( node, depth )
    if node is terminal or depth == 0
        return heuristic value of node
        // from perspective of player whose turn it is to move
    let α = − ∞
    foreach child of node
        let α = max( α, -negamax( child, depth-1 ))
    return α
```

### Alpha-Beta Pruning

- The minimax algorithm searches all nodes in the game tree exhaustively.
- We can speed up the search by evaluating only those nodes that are needed, and "pruning" the others.
- **Alpha-Beta Algorithm**: At each node of the tree, we keep track of two values α and β such that the current node will only be relevant if its evaluation is between α and β.
- This algorithm is guaranteed to give the same answer as Minimax search, but the code runs much faster.

```c
function alphabeta( node, depth, α, β )
    if node is a terminal node or depth == 0
        return heuristic value of node
    if we are to play at node
        foreach child of node
            let α = max( α, alphabeta( child, depth-1, α, β ))
            if α ≥ β
                return α
        return α
    else// opponent is to play at node
        foreach child of node
            let β = min( β, alphabeta( child, depth-1, α, β ))
            if β ≤ α
                return β
        return β

```

- Negamax formulation of alpha-beta search:

```c
function minimax( node, depth )
    return alphabeta( node, depth, −∞, ∞ )
function alphabeta( node, depth, α, β )
    if node is terminal or depth == 0
        return heuristic value of node
        // from perspective of player whose turn it is to move
    foreach child of node
        let α = max( α, -alphabeta( child, depth-1, -β, -α ))
        if α ≥ β
            return α
    return α
```

|  | Minimax | Alpha-Beta Pruning |
| --- | --- | --- |
| Time | O(b^m) | O(b^m/2) |
| Space | O(bm) |  |
| Complete? | Yes, if tree is finite | Yes, if tree is finite |
| Optimal | Yes, against an optimal opponent | Yes, against an optimal opponent |

### Expectimax

- These are used for stochastic games (games with an element of randomness)
- Introduces a chance node into the minimax algorithm.

![Untitled]({{ "/images/comp3411images/Untitled%2017.png" | absolute_url }})

- The chance node is the expected value of its child nodes.
- At chance nodes the outcome is not chosen by either player but instead determined randomly.
- Exact values:
    - In minimax search, the exact numerical do not matter - only their ordering matters.
    - For expectimax, exact values DO matter.
    - Therefore, for stochastic games we typically have our **heuristic evaluation as a probability between 0 and 1** that is interpreted as the ‘probability of winning’
    - In practice, this is usually achieved by taking the linear function above and composing it with a (monotonic) sigmoidal function

---

# Week 4

- Types of Learning:
    - **Supervised learning** - Agent is presented with examples of inputs and their target outputs
    - **Reinforcement learning** - Agent is not presented with target outputs, but is given a reward signal, which it aims to maximise.
    - **Unsupervised Learning** - Agent is only presented with the inputs themselves, and aims to find structure in these inputs
- Supervised Learning:
    - We have a training set and a test set, each consisting of a set of items; for each item, a number of input attributes and a target value are specified.
    - The aim is to predict the target value, based on the input attributes.
    - Agent is presented with the input and target output for each item in the training set; it must then predict the output for each item in the test set
- When fitting a curve to a set of data points we use Ockham’s Razor.
- Ockham’s Razor:
    
    > **“The most likely hypothesis is the simplest one consistent with the data.”**
    > 

![Untitled]({{ "/images/comp3411images/Untitled%2018.png" | absolute_url }})

### Decision Trees

- **Example**: Restaurant Waiting

![Untitled]({{ "/images/comp3411images/Untitled%2019.png" | absolute_url }})

- The task is to predict whether or not someone will Wait for a table at a restaurant, given these attributes:
    - **Alt** - is there a suitable alternative restaurant nearby?
    - **Bar** - does the restaurant have a bar area to wait in?
    - **F/S** - is it a Friday or Saturday?
    - **Hun** - is the person hungry?
    - **Pat** - how many patrons are currently in the restaurant?
    - **Price** - is it cheap, medium, expensive?
    - **Rain** - is it raining outside?
    - **Res** - have they made a reservation?
    - **Type** - type of food served in the restaurant.
    - **Est** - estimated waiting time in minutes

- One potential decision tree is:

![Untitled]({{ "/images/comp3411images/Untitled%2020.png" | absolute_url }})

- Generalisation:
    - Provided the training data are not **inconsistent**, we can split the attributes in any order and still produce a tree that correctly classifies all examples in the training set.
    - However, we really want a tree which is likely to **generalise** to correctly classify the (unseen) examples in the **test set**.
    - In view of Ockham’s Razor, we prefer a **simpler** hypothesis, i.e. a smaller tree.
    - But how can we choose attributes in order to produce a small tree?
- Choosing an attribute:
    - Patrons is a “more informative” attribute than Type, because it splits the examples more nearly into sets that are “all positive” or “all negative”.
    - This notion of “informativeness” can be quantified using the mathematical concept of “entropy”.
    - A parsimonious tree can be built by minimising the entropy at each step.

### Entropy

- Entropy is a measure of how much information we **gain** when the target attribute is revealed to us. In other words, it is not a measure of how much we know, but of how much we **don’t** know.

![Untitled]({{ "/images/comp3411images/Untitled%2021.png" | absolute_url }})

![Untitled]({{ "/images/comp3411images/Untitled%2022.png" | absolute_url }})

- Example:

![Untitled]({{ "/images/comp3411images/Untitled%2023.png" | absolute_url }})

### Decision Tree Pruning

- According to Ockham’s Razor, we may wish to prune off branches that do not provide much benefit in classifying the items.
- When a node becomes a leaf, all items will be assigned to the majority class at that node. We can estimate the error rate on the (unseen) test items using the **Laplace error**:
    
    ![Untitled]({{ "/images/comp3411images/Untitled%2024.png" | absolute_url }})
    
    - N = total number of training items at the node
    - n = number of training items in the majority class
    - k = number of classes
- If the average Laplace error of the children exceeds that of the parent node, we prune off the children

### Neurons - Biological

![Untitled]({{ "/images/comp3411images/Untitled%2025.png" | absolute_url }})

- The outermost layer of the brain, known as the **cortex**, is what distinguishes humans from other animals.
- It is divided into four lobes:
    - Frontal
    - Parietal
    - Occipital
    - Temporal
- Other regions such as the Cerebellum and Brain stem are much older, and control essential functions such as breathing, balance and reflex actions.

![Untitled]({{ "/images/comp3411images/Untitled%2026.png" | absolute_url }})

- In general, cognitive functions are not precisely localised in specific regions. However, sensory and motor functions are generally at the boundary between the Parietal and Frontal lobes, and the Primary Visual cortex is located at the back of the head.
- **The structure of a neuron:**

![Untitled]({{ "/images/comp3411images/Untitled%2027.png" | absolute_url }})

- The brain is made up of neurons which have:
    - a cell body (soma)
    - dendrites (inputs)
    - an axon (outputs)
    - synapses (connections between cells)
- Synapses can be **exibatory** or **inhibitory** and may change over time.
- When the inputs reach some threshold, an action potential (electrical pulse) is sent along the axon to the outputs. The axon can connect via a synapse to the dendrites of other neurons, leading to a chain reaction.

- Facts about the brain's computational power:
    - The human brain has 100 billion neurons with an average of 10,000 synapses each
    - Latency is about 3-6 milliseconds
- In contrast to a traditional computer program executing a series of instructions one after the other in microseconds, the brain performs millions of computations simultaneously, but each one takes on the order of 3-6 milliseconds.
- Therefore, there are at most a few hundred “steps” in any mental computation, but the computation is massively parallel.

### Neurons - Artificial

- Artificial neural networks are made up of nodes which have the following:
    - inputs edges, each with some weight
    - one or more outputs edges
    - an activation level (a function of the inputs)
- Weights can be positive or negative and may change over time (learning).
- The input function is the weighted sum of the activation levels of inputs.
- The activation level is a non-linear transfer function *g*() of this (linear) input:

![Untitled]({{ "/images/comp3411images/Untitled%2028.png" | absolute_url }})

- Some nodes are inputs (sensing) and some are outputs (action)

### **McCulloch & Pitts Model of a Single Neuron**

![Untitled]({{ "/images/comp3411images/Untitled%2029.png" | absolute_url }})

- Transfer function:
    - Originally, a discontinuous step function was used as the transfer function
    
    ![Untitled]({{ "/images/comp3411images/Untitled%2030.png" | absolute_url }})
    
    ![Untitled]({{ "/images/comp3411images/Untitled%2031.png" | absolute_url }})
    

### Perceptron Learning

- The above approach works fine if there are a small number of inputs. However, what if there were 100, or even 1000 inputs? We can use perceptron learning to automatically determine weights for a large network, based on a set of training data.
- **Idea**: Adjust the weights as each input is presented
    
    ![Untitled]({{ "/images/comp3411images/Untitled%2032.png" | absolute_url }})
    

### Neural Networks

- Perceptrons only work for linearly separable functions, but many useful functions are not linearly separable.

![Untitled]({{ "/images/comp3411images/Untitled%2033.png" | absolute_url }})

- **Possible solution**: Rewrite a given logical expression in terms of simpler functions such as AND, OR and NOR, which can each be implemented by a single neuron. Then build a network of neurons to implement this combination.
    
    ![Untitled]({{ "/images/comp3411images/Untitled%2034.png" | absolute_url }})
    
- Two-layer Neural Network:
    
    ![Untitled]({{ "/images/comp3411images/Untitled%2035.png" | absolute_url }})
    
- Normally, the number of input and output units are fixed, but we can choose the number of hidden nodes.

…

---

# Week 5

### Uncertainty

- There are many situations where an agent has to choose an action based on incomplete information.
- This is true, for example if the environment is **stochastic**. In the game of Backgammon, we do not know what the next dice roll will be.
- Another reason is **partial observability**. Some aspects of the environment may be hidden from the agent altogether.
- In addition, robot sensors are often **noisy** in the sense that the quantities observed by the robot differ to some extent from their "true" values (due to factors which the agent is unable to observe).
- Utility theory:
    - Used to represent and infer preferences
- Decision theory:
    - Utility theory + Probability theory

### Reinforcement Learning

- Types of learning:
    
    ![Untitled]({{ "/images/comp3411images/Untitled%2036.png" | absolute_url }})
    
- **Supervised learning:**
    - We have a training set and a test set, each consisting of a set of examples. For each example, a number of input attributes and a target attribute are specified.
    - The aim is to predict the target attribute, based on the input attributes.
    - Various learning paradigms are available, such as **Decision Trees** and **Neural Networks**
- Reinforcement Learning Framework:
    
    ![Untitled]({{ "/images/comp3411images/Untitled%2037.png" | absolute_url }})
    
- Elements of Reinforcement Learning:
    - **Policy** - Informs how to act in a particular situation. A set of stimulus-response rules.
    - **Reward Function** - Defines the aim of an RL problem. Maps each perceived state into a number, the reward. Goal is to maximise the long-term reward.
    - **Value Function** - Shows what is good in the long run. Actions are decided based on the value.
    - **Environment Model** (Optional) - Imitates the environment behaviour. Can predict states and rewards obtained.
- Exploration vs Exploitation Trade-off
    - Most of the time the agent chooses what it thinks the best action is.
    - But to learn, it must occasionally choose something different from the preferred action.
    - **Exploitation** maximises the immediate reward and **exploration** is better for the long-run.
- The agent-environment interface:
    
    ![Untitled]({{ "/images/comp3411images/Untitled%2038.png" | absolute_url }})
    
- Goals and rewards:
    - A chess player should be rewarded only for winning and not taking opponent’s pieces.
    - Otherwise the agent will learn to maximise subgoals.
    - The reward signal is the way to communicate to the agent WHAT you want it to achieve, not HOW you want it achieved.